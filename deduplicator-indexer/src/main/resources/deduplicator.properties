# Properties file for DeDuplicator
# All the key values here will be loaded into System.properties and thus made available to
# third party CrawlDataIterators and/or RevisitResolvers

# DeDuplicator settings. 
# These are default values for things that can also be set via the command line.
# These values are used if nothing is specified on the command line

# Index by URL, DIGEST or BOTH
deduplicator.mode=BOTH
# Use a canonicalized URL in the index
deduplicator.canonicalurl=true
# Include the ETag from the HTTP header, if available, in the index. 
# Used for Server-Not-Modified detection (comming soon)
deduplicator.etag=false
# A filter on what mime types are added into the index. It acts as a blacklist, unless whitelist is set to true.
deduplicator.mime=^text/.*
# Make the mime filter a whitelist instead of blacklist.
deduplicator.whitelist=false
# If false, any existing index in target directory will be overwritten. If true, will add to the index.
deduplicator.add=false
# CrawlDataIterator. Processes source data into CrawlDataItems that can be added to the index.
# The default, WarcIterator, excepts the source field to be a directory containing WARCs 
# it will process that directory recursively
deduplicator.crawldataiterator=is.landsbokasafn.deduplicator.indexer.WarcIterator
# Alternative iterator over a Heritrix crawl.log. Wont index item marked as revisits in the crawl log unless
# there is a revisit resolver provided
#deduplicator.crawldataiterator=is.landsbokasafn.deduplicator.indexer.CrawlLogIterator
# By default, the CrawlLogIterator looks for the revisit annotations written by the DeDuplicator module,
# for alternative options, provide a regular expression matching the annotation field here
#deduplicator.crawllogiterator.revisit-annotation-regex=

# Specify a revisit resolver that determines the original URL and timestamp of a revisit item found that did
# not contain information about original capture. 
# deduplicator.revistresolver=

# ===========

